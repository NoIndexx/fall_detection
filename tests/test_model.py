import pytest
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler

from model import split_data, train_random_forest, predict, TARGET_COLUMN, FEATURE_COLUMNS_TO_DROP

@pytest.fixture
def sample_features_df():
    # Create a sample DataFrame that mimics the output of feature_engineering
    num_rows = 100
    num_features = 132 # As generated by feature_engineering (12 signal * 11 stats)
    data = np.random.rand(num_rows, num_features)
    columns = [f'feature_{i}' for i in range(num_features)]
    df = pd.DataFrame(data, columns=columns)
    df['trial_id'] = [f'trial_{i//10}' for i in range(num_rows)]
    df['label'] = np.random.randint(0, 2, num_rows) # Binary labels 0 or 1
    return df

def test_split_data(sample_features_df):
    test_size = 0.25
    X_train, X_test, y_train, y_test, scaler = split_data(sample_features_df, test_size=test_size, random_state=42)
    
    assert X_train.shape[0] == int(len(sample_features_df) * (1 - test_size))
    assert X_test.shape[0] == int(len(sample_features_df) * test_size)
    assert y_train.shape[0] == X_train.shape[0]
    assert y_test.shape[0] == X_test.shape[0]
    
    # Check that original 'label' and 'trial_id' are not in X_train/X_test
    for col_to_drop in FEATURE_COLUMNS_TO_DROP:
        assert col_to_drop not in X_train.columns
        assert col_to_drop not in X_test.columns
        
    assert isinstance(scaler, StandardScaler)

    # Check for stratification (approximate check for balanced labels in splits if original is balanced)
    # More rigorous check would require known imbalanced data
    original_label_ratio = sample_features_df[TARGET_COLUMN].value_counts(normalize=True)
    train_label_ratio = y_train.value_counts(normalize=True)
    test_label_ratio = y_test.value_counts(normalize=True)
    
    # Allow for some tolerance due to small sample size and rounding
    if 0 in original_label_ratio and 0 in train_label_ratio:
      pd.testing.assert_series_equal(original_label_ratio, train_label_ratio, check_dtype=False, atol=0.1)
      pd.testing.assert_series_equal(original_label_ratio, test_label_ratio, check_dtype=False, atol=0.1)

def test_train_random_forest(sample_features_df):
    X_train, _, y_train, _, _ = split_data(sample_features_df, test_size=0.2, random_state=42)
    model = train_random_forest(X_train, y_train, n_estimators=10, random_state=42, class_weight='balanced')
    
    assert isinstance(model, RandomForestClassifier)
    assert hasattr(model, 'classes_') # A simple check to see if the model has been fitted
    assert model.n_estimators == 10

def test_predict(sample_features_df):
    X_train, X_test, y_train, _, _ = split_data(sample_features_df, test_size=0.2, random_state=42)
    model = train_random_forest(X_train, y_train, n_estimators=10, random_state=42)
    
    predictions, predict_proba = predict(model, X_test)
    
    assert len(predictions) == len(X_test)
    assert len(predict_proba) == len(X_test)
    assert predict_proba.shape[1] == 2 # Probabilities for 2 classes
    assert all(pred_val in [0, 1] for pred_val in predictions) # Predictions should be 0 or 1

# Test for NaN handling in split_data
@pytest.fixture
def sample_features_df_with_nan():
    num_rows = 10
    num_features = 5 
    data = np.random.rand(num_rows, num_features)
    columns = [f'feature_{i}' for i in range(num_features)]
    df = pd.DataFrame(data, columns=columns)
    df['trial_id'] = 'trial1'
    df['label'] = np.random.randint(0, 2, num_rows)
    # Introduce NaNs
    df.iloc[0, 0] = np.nan
    df.iloc[1, 2] = np.nan
    return df

def test_split_data_nan_handling(sample_features_df_with_nan):
    X_train, X_test, y_train, y_test, scaler = split_data(sample_features_df_with_nan, test_size=0.2, random_state=42)
    
    # Check that NaNs are filled (no NaNs in X_train or X_test after scaling)
    assert not X_train.isnull().sum().any()
    assert not X_test.isnull().sum().any() 